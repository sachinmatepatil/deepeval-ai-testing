# ğŸ§ª DeepEval AI Testing Project â€“ Day 1

This project uses [DeepEval](https://github.com/confident-ai/deepeval) to test the output of LLMs using the `FaithfulnessMetric`.

## âœ… What This Does

- Sends a question to the LLM
- Compares actual vs expected response
- Scores how faithful the response is to a known context
- Prints score (e.g., âœ… PASSED if score > 0.7)

## ğŸ§  Prompt

**Input:** What is an API?  
**Actual Output:** An API lets software talk.  
**Expected Output:** An API allows two apps to communicate.  
**Context:** API stands for Application Programming Interface.

## ğŸš€ How to Run

1. Install dependencies:
pip install -r requirements.txt
2. Add your OpenAI API key to your environment variables:
3. Run the script:


## ğŸ“˜ Day 2 â€” Understanding LLM Output Randomness

### ğŸ” Objective
To demonstrate how Large Language Models (LLMs) like GPT-3.5 can produce **non-deterministic outputs** when generating text â€” even for the same prompt.

---

### âš™ï¸ What This Script Does

- Calls an OpenAI LLM twice with the **same prompt**
- Uses a moderate temperature (e.g., 0.9) to allow variation
- Prints both outputs
- Shows the differences using Pythonâ€™s `unified_diff`

---

### ğŸ§ª Prompt Used

```
Explain what API testing is in one short paragraph.
```

---

### ğŸ§  Key Concepts Learned

| Concept           | Description                                                                 |
|------------------|-----------------------------------------------------------------------------|
| ğŸ” Non-determinism | LLMs generate different outputs due to randomness at higher temperatures     |
| ğŸ”¥ Temperature      | Controls randomness â€” higher = more creative, lower = more consistent        |
| ğŸ§® Tokens           | Words are broken into smaller chunks (tokens) during processing              |
| ğŸ“‰ Top-p            | Nucleus sampling â€” selects from top % of most likely tokens                 |
| ğŸ¤¯ Hallucination    | When LLM gives factually incorrect answers confidently                      |

---

### ğŸ§µ Sample Output (partial)

```diff
- API testing ensures that APIs return correct data and follow expected behavior.
+ API testing validates endpoints by sending requests and verifying responses.
```

---

ğŸ§  Day 3 â€” Built-in + Custom LLM Metrics (DeepEval)

ğŸ—‚ï¸ File: day3_llm_eval.py
ğŸ¯ Goal: Evaluate LLM output using:

âœ… AnswerRelevanceMetric (checks if the answer is relevant)

âœ… ContextPrecisionMetric (checks if answer is grounded in context)

ğŸ› ï¸ FactualAccuracyMetric (custom rule-based metric)

ğŸ” Prompt Tested
Input:        "What is the capital of France?"
Expected:     "Paris is the capital of France."
Actual:       "The capital of France is Paris."
Context:      ["France is a country in Europe. Paris is its capital."]

ğŸ“Š Output Sample (Console)
Metric: Answer Relevance | Score: âœ… 1.0 | Status: PASSED
Metric: Context Precision | Score: âœ… 1.0 | Status: PASSED
Metric: Factual Accuracy (custom) | Score: âœ… 1.0 | Status: PASSED

ğŸ§ª Custom Metric Logic

Simple rule: If "paris" is present in the actual answer â†’ score = 1.0

This simulates a factual validation check (can be extended with NLP logic or regex)

ğŸ’¡ What I Learned

How to run multiple evaluation metrics together

How to create my own BaseMetric subclass in DeepEval

Hands-on practice of LLM output validation techniques used in real-world GenAI products

----------------------------------------------------